{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jpawitro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jpawitro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import string\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "listStopword =  set(stopwords.words('indonesian'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = os.path.join(\"..\",\"data\")\n",
    "model = os.path.join(\"..\",\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "file_slang  = open(os.path.join(\"..\",\"data\",\"cleaning_source\",\"update_combined_slang_words.txt\"), \"r\")\n",
    "content = file_slang.read()\n",
    "slang_words = ast.literal_eval(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pickle.load(open(os.path.join(model,\"randomforest.sav\"),\"rb\"))\n",
    "le = pickle.load(open(os.path.join(model,\"le.sav\"),\"rb\"))\n",
    "dfeat = pd.read_json(os.path.join(model,\"feature.json\")).sort_index()\n",
    "ftmod = FastText.load(os.path.join(model,\"fasttext.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fida310</td>\n",
       "      <td>MaasyaaAllah si kaseep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nrxzra_</td>\n",
       "      <td>Buat yang komen jahat, semoga balik ke diri se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxyouria_</td>\n",
       "      <td>kok orang2 jahat banget ya ketikannya, ga nger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>astriivo08</td>\n",
       "      <td>Kok lama lama mirip Tukul ya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>taldv_</td>\n",
       "      <td>Ksini krn Twitter, I'm not a leslar fans, tapi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>somedayssi17</td>\n",
       "      <td>Ih warga ig serem serem trnyata komenannya smg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mirnha10_</td>\n",
       "      <td>Ku kira monyet ternayata dajal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vn_rajjjj</td>\n",
       "      <td>Muka lesty kayak hantu di film IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>murni_sulistiani26</td>\n",
       "      <td>Kok tambah punya anak tambah jelek sihâ€¦â€¦.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fanykandow</td>\n",
       "      <td>Mirip daus mini anaknya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ismi_wadidaw2404</td>\n",
       "      <td>Ko gtu sih muka baby nya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mommy_air9</td>\n",
       "      <td>Jelek bgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rajja9296</td>\n",
       "      <td>Udah kayak anjing kau kayak gitu nggos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nafilahaziz</td>\n",
       "      <td>Semangatt teruss yaa inshaAllah suatu hari aka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>yusuppp.23</td>\n",
       "      <td>Dia bermaksud buat ngehibur, klo kalian ga mer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>joovi_ii</td>\n",
       "      <td>Sakit sakit ni orang dikomen, semoga sadar ket...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>shandy1335</td>\n",
       "      <td>Kayak anak terbelakangan mental. Bnyak halu ny...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rizaldy4689</td>\n",
       "      <td>Kakak kok jelek sih?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>_wayba</td>\n",
       "      <td>Hidup jangan saling menjatuhkan kakðŸ˜­</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>andramol123</td>\n",
       "      <td>Caption sama mukanya kok gak pantes yah kalo d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  User                                            Comment\n",
       "0              fida310                             MaasyaaAllah si kaseep\n",
       "1              nrxzra_  Buat yang komen jahat, semoga balik ke diri se...\n",
       "2            xxyouria_  kok orang2 jahat banget ya ketikannya, ga nger...\n",
       "3           astriivo08                       Kok lama lama mirip Tukul ya\n",
       "4               taldv_  Ksini krn Twitter, I'm not a leslar fans, tapi...\n",
       "5         somedayssi17  Ih warga ig serem serem trnyata komenannya smg...\n",
       "6            mirnha10_                     Ku kira monyet ternayata dajal\n",
       "7            vn_rajjjj                  Muka lesty kayak hantu di film IT\n",
       "8   murni_sulistiani26      Kok tambah punya anak tambah jelek sihâ€¦â€¦.\n",
       "9           fanykandow                            Mirip daus mini anaknya\n",
       "10    ismi_wadidaw2404                           Ko gtu sih muka baby nya\n",
       "11          mommy_air9                                          Jelek bgt\n",
       "12           rajja9296             Udah kayak anjing kau kayak gitu nggos\n",
       "13         nafilahaziz  Semangatt teruss yaa inshaAllah suatu hari aka...\n",
       "14          yusuppp.23  Dia bermaksud buat ngehibur, klo kalian ga mer...\n",
       "15            joovi_ii  Sakit sakit ni orang dikomen, semoga sadar ket...\n",
       "16          shandy1335  Kayak anak terbelakangan mental. Bnyak halu ny...\n",
       "17         rizaldy4689                               Kakak kok jelek sih?\n",
       "18              _wayba            Hidup jangan saling menjatuhkan kakðŸ˜­\n",
       "19         andramol123  Caption sama mukanya kok gak pantes yah kalo d..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(data,\"Data_inferences.csv\"),sep=\",\", encoding='cp1252')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data, listtoberemoved = []):\n",
    "    cleaned = []\n",
    "    for n in data:\n",
    "        n = p.clean(n)\n",
    "        n = n.lower()\n",
    "        n = re.sub(r':', '', n)\n",
    "        n = re.sub(r'‚Ä¶', '', n)\n",
    "        n = re.sub(r'[^\\x00-\\x7F]+',' ', n)\n",
    "        n = emoji_pattern.sub(r'', n)\n",
    "        n = re.sub('[^a-zA-Z]', ' ', n)\n",
    "        n = re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",n)\n",
    "        n = re.sub(\"(\\\\d|\\\\W)+\",\" \",n)\n",
    "        n = re.sub(r'â', '', n)\n",
    "        n = re.sub(r'€', '', n)\n",
    "        n = re.sub(r'¦', '', n)\n",
    "        cleaned.append(n)\n",
    "\n",
    "    tokenized = []\n",
    "    for n in cleaned:\n",
    "        n = word_tokenize(n)\n",
    "        for w in n:\n",
    "            if w in slang_words.keys():\n",
    "                n[n.index(w)] = slang_words[w]\n",
    "        tokenized.append(n)\n",
    "\n",
    "    removed = []\n",
    "    for ts in tokenized:\n",
    "        n = []\n",
    "        for t in ts:\n",
    "            if t not in listtoberemoved and t not in listStopword and t not in emoticons and t not in string.punctuation:\n",
    "                n.append(t)\n",
    "        removed.append(n)\n",
    "\n",
    "    stemmed = []\n",
    "    for n in removed:\n",
    "        n = ' '.join(n)\n",
    "        n = stemmer.stem(n)\n",
    "        n = n.split(' ')\n",
    "        stemmed.append(n)\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequencer():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 all_words,\n",
    "                 max_words,\n",
    "                 seq_len,\n",
    "                 embedding_matrix\n",
    "                ):\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.embed_matrix = embedding_matrix\n",
    "\n",
    "        temp_vocab = list(set(all_words))\n",
    "        self.vocab = []\n",
    "        self.word_cnts = {}\n",
    "\n",
    "        for word in temp_vocab:\n",
    "            count = len([0 for w in all_words if w == word])\n",
    "            self.word_cnts[word] = count\n",
    "            counts = list(self.word_cnts.values())\n",
    "            indexes = list(range(len(counts)))\n",
    "        \n",
    "        cnt = 0\n",
    "        while cnt + 1 != len(counts):\n",
    "            cnt = 0\n",
    "            for i in range(len(counts)-1):\n",
    "                if counts[i] < counts[i+1]:\n",
    "                    counts[i+1],counts[i] = counts[i],counts[i+1]\n",
    "                    indexes[i],indexes[i+1] = indexes[i+1],indexes[i]\n",
    "                else:\n",
    "                    cnt += 1\n",
    "        \n",
    "        for ind in indexes[:max_words]:\n",
    "            self.vocab.append(temp_vocab[ind])\n",
    "                    \n",
    "    def textToVector(self,text):\n",
    "        tokens = text.split()\n",
    "        len_v = len(tokens)-1 if len(tokens) < self.seq_len else self.seq_len-1\n",
    "        vec = []\n",
    "        for tok in tokens[:len_v]:\n",
    "            try:\n",
    "                vec.append(self.embed_matrix[tok])\n",
    "            except Exception as E:\n",
    "                pass\n",
    "        \n",
    "        last_pieces = self.seq_len - len(vec)\n",
    "        for i in range(last_pieces):\n",
    "            vec.append(np.zeros(100,))\n",
    "        \n",
    "        return np.asarray(vec).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = preprocessing(df['Comment'].tolist())\n",
    "\n",
    "sequencer = Sequencer(all_words = [token for seq in prep for token in seq],\n",
    "          max_words = 1200,\n",
    "          seq_len = 15,\n",
    "          embedding_matrix = ftmod.wv\n",
    "        )\n",
    "\n",
    "x_vecs = np.asarray([sequencer.textToVector(\" \".join(seq)) for seq in prep])\n",
    "X = x_vecs[:,dfeat.index]\n",
    "X = pd.DataFrame(X)\n",
    "pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fida310</td>\n",
       "      <td>MaasyaaAllah si kaseep</td>\n",
       "      <td>Non-bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nrxzra_</td>\n",
       "      <td>Buat yang komen jahat, semoga balik ke diri se...</td>\n",
       "      <td>Non-bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxyouria_</td>\n",
       "      <td>kok orang2 jahat banget ya ketikannya, ga nger...</td>\n",
       "      <td>Non-bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>astriivo08</td>\n",
       "      <td>Kok lama lama mirip Tukul ya</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>taldv_</td>\n",
       "      <td>Ksini krn Twitter, I'm not a leslar fans, tapi...</td>\n",
       "      <td>Non-bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>somedayssi17</td>\n",
       "      <td>Ih warga ig serem serem trnyata komenannya smg...</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mirnha10_</td>\n",
       "      <td>Ku kira monyet ternayata dajal</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vn_rajjjj</td>\n",
       "      <td>Muka lesty kayak hantu di film IT</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>murni_sulistiani26</td>\n",
       "      <td>Kok tambah punya anak tambah jelek sihâ€¦â€¦.</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fanykandow</td>\n",
       "      <td>Mirip daus mini anaknya</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ismi_wadidaw2404</td>\n",
       "      <td>Ko gtu sih muka baby nya</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mommy_air9</td>\n",
       "      <td>Jelek bgt</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rajja9296</td>\n",
       "      <td>Udah kayak anjing kau kayak gitu nggos</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nafilahaziz</td>\n",
       "      <td>Semangatt teruss yaa inshaAllah suatu hari aka...</td>\n",
       "      <td>Non-bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>yusuppp.23</td>\n",
       "      <td>Dia bermaksud buat ngehibur, klo kalian ga mer...</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>joovi_ii</td>\n",
       "      <td>Sakit sakit ni orang dikomen, semoga sadar ket...</td>\n",
       "      <td>Non-bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>shandy1335</td>\n",
       "      <td>Kayak anak terbelakangan mental. Bnyak halu ny...</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rizaldy4689</td>\n",
       "      <td>Kakak kok jelek sih?</td>\n",
       "      <td>Non-bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>_wayba</td>\n",
       "      <td>Hidup jangan saling menjatuhkan kakðŸ˜­</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>andramol123</td>\n",
       "      <td>Caption sama mukanya kok gak pantes yah kalo d...</td>\n",
       "      <td>Bully</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  User                                            Comment  \\\n",
       "0              fida310                             MaasyaaAllah si kaseep   \n",
       "1              nrxzra_  Buat yang komen jahat, semoga balik ke diri se...   \n",
       "2            xxyouria_  kok orang2 jahat banget ya ketikannya, ga nger...   \n",
       "3           astriivo08                       Kok lama lama mirip Tukul ya   \n",
       "4               taldv_  Ksini krn Twitter, I'm not a leslar fans, tapi...   \n",
       "5         somedayssi17  Ih warga ig serem serem trnyata komenannya smg...   \n",
       "6            mirnha10_                     Ku kira monyet ternayata dajal   \n",
       "7            vn_rajjjj                  Muka lesty kayak hantu di film IT   \n",
       "8   murni_sulistiani26      Kok tambah punya anak tambah jelek sihâ€¦â€¦.   \n",
       "9           fanykandow                            Mirip daus mini anaknya   \n",
       "10    ismi_wadidaw2404                           Ko gtu sih muka baby nya   \n",
       "11          mommy_air9                                          Jelek bgt   \n",
       "12           rajja9296             Udah kayak anjing kau kayak gitu nggos   \n",
       "13         nafilahaziz  Semangatt teruss yaa inshaAllah suatu hari aka...   \n",
       "14          yusuppp.23  Dia bermaksud buat ngehibur, klo kalian ga mer...   \n",
       "15            joovi_ii  Sakit sakit ni orang dikomen, semoga sadar ket...   \n",
       "16          shandy1335  Kayak anak terbelakangan mental. Bnyak halu ny...   \n",
       "17         rizaldy4689                               Kakak kok jelek sih?   \n",
       "18              _wayba            Hidup jangan saling menjatuhkan kakðŸ˜­   \n",
       "19         andramol123  Caption sama mukanya kok gak pantes yah kalo d...   \n",
       "\n",
       "        Class  \n",
       "0   Non-bully  \n",
       "1   Non-bully  \n",
       "2   Non-bully  \n",
       "3       Bully  \n",
       "4   Non-bully  \n",
       "5       Bully  \n",
       "6       Bully  \n",
       "7       Bully  \n",
       "8       Bully  \n",
       "9       Bully  \n",
       "10      Bully  \n",
       "11      Bully  \n",
       "12      Bully  \n",
       "13  Non-bully  \n",
       "14      Bully  \n",
       "15  Non-bully  \n",
       "16      Bully  \n",
       "17  Non-bully  \n",
       "18      Bully  \n",
       "19      Bully  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.assign(Class = pd.DataFrame(le.inverse_transform(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(text):\n",
    "    text = [text]\n",
    "    prep = preprocessing(text)\n",
    "    sequencer = Sequencer(all_words = [token for seq in prep for token in seq],\n",
    "            max_words = 1200,\n",
    "            seq_len = 15,\n",
    "            embedding_matrix = ftmod.wv\n",
    "            )\n",
    "\n",
    "    x_vecs = np.asarray([sequencer.textToVector(\" \".join(seq)) for seq in prep])\n",
    "    X = x_vecs[:,dfeat.index]\n",
    "    X = pd.DataFrame(X)\n",
    "    pred = clf.predict(X)\n",
    "    return le.inverse_transform(pred)[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bully'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"jelek amat muka lu\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb46487324caf9390db8420660f238fdf08c127ee7c96757106614c8aa595731"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
